{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50b550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "# read image\n",
    "path = 'tr_img_09001.jpg'\n",
    "path1 = '0.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f4f9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_xx = [[217,37,580,42,591,147,222,133], [221,133,565,144,574,221,216,223]]\n",
    "list_xy = [[122,294,295,126,207,214,277,270], [291,469,472,303,213,221,283,287]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359205c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bb_coordinates_xx_xy(list_xx_xy, format_ = None):\n",
    "    out_list = []\n",
    "    for index, list_ in enumerate(list_xx_xy):\n",
    "        temp_dict = {}\n",
    "        if format_ == 'xx':\n",
    "            x1, x2, x3, x4, y1, y2, y3, y4 = list_\n",
    "            \n",
    "        elif format_ == 'xy':\n",
    "            x1, y1, x2, y2, x3, y3, x4, y4 = list_\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"format either has to be 'xx' or 'xy'\")\n",
    "        temp_dict[\"x_min\"] = int((x1 + x4)/2)\n",
    "        temp_dict[\"x_max\"] = int((x2 + x3)/2)\n",
    "        temp_dict[\"y_min\"] = int((y1 + y2)/2)\n",
    "        temp_dict[\"y_max\"] = int((y3 + y4)/2)\n",
    "        \n",
    "        #temp_dict[\"index\"] = [x_min, x_max, y_min, y_max]\n",
    "        out_list.append(temp_dict)\n",
    "    return out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3659b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_c_train = bb_coordinates_xx_xy(list_xx, 'xy')\n",
    "bb_c_val = bb_coordinates_xx_xy(list_xy, 'xx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287d785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_image(img, all_annotations):\n",
    "    for annotation in all_annotations:\n",
    "        draw_bounding_box(img, annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eda018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    img_path = path\n",
    "    img = cv2.imread(img_path)\n",
    "    return img\n",
    "def plot_image(img):\n",
    "    plt.imshow(img)\n",
    "    plt.title(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1df013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bounding_box(img, annotation):\n",
    "    \n",
    "    x_min, y_min = int(annotation['x_min']), int(annotation['y_min'])\n",
    "    x_max, y_max = int(annotation['x_max']), int(annotation['y_max'])\n",
    "    color = class_to_color(0)\n",
    "    \n",
    "    cv2.rectangle(img,(x_min,y_min),(x_max,y_max), color, 2)\n",
    "    \n",
    "def class_to_color(class_id):\n",
    "    colors = [(255,0,0),(0,255,0),(0,0,255),(255,255,0),(255,0,255),(0,255,255),(255,100,100),\n",
    "              (100,255,100),(100,100,255),(255,100,0),(255,0,100),(100,0,255),(100,100,255),(100,255,0),\n",
    "              (100,255,100)]\n",
    "    return colors[class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc16e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_image(path)\n",
    "annotate_image(img, bb_c_train)\n",
    "plot_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce05b731",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_image(path1)\n",
    "annotate_image(img, bb_c_val)\n",
    "plot_image(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5b1088",
   "metadata": {},
   "source": [
    "**YOLO Annotation Type Conversion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63598a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Pascal_Voc bb to Yolo\n",
    "def pascal_voc_to_yolo(x_min, y_min, x_max, y_max, image_w, image_h):\n",
    "    \n",
    "    class_id = 0\n",
    "    pw = x_max - x_min\n",
    "    ph = y_max - y_min\n",
    "    cx = x_min + pw/2\n",
    "    cy = y_min + ph/2\n",
    "    NorX = cx / image_w\n",
    "    NorY = cy / image_h\n",
    "    NorW = pw / image_w\n",
    "    NorH = ph / image_h\n",
    "    \n",
    "    return (class_id, NorX, NorY, NorW, NorH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78313ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pascal_voc_to_yolo(x_min, y_min, x_max, y_max, image_w, image_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021961c2",
   "metadata": {},
   "source": [
    "**Converting annotations for a folder of images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821d65f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "images_folder = r\"D:\\PyTorch\\Padhai\\Capstone\\Incidental_Scene\\Text Localization\\valid\\images\"\n",
    "labels_folder = r\"D:\\PyTorch\\Padhai\\Capstone\\Incidental_Scene\\Text Localization\\ch4_test_gt\"\n",
    "labels_yolo = r\"D:\\PyTorch\\Padhai\\Capstone\\Incidental_Scene\\Text Localization\\valid\\labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f8eca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(images_folder):\n",
    "    img = cv2.imread(os.path.join(images_folder, filename))\n",
    "    image_h, image_w, _ = img.shape\n",
    "    \n",
    "    name= filename.split('.jpg')\n",
    "    dest_file = os.path.join(labels_yolo,f\"{name[0]}.txt\")\n",
    "    file = open(dest_file,'w')\n",
    "    src_fullname = os.path.join(labels_folder, f'gt_{name[0]}.txt')\n",
    "    src_label_file = open(src_fullname, 'r', encoding='utf-8-sig')\n",
    "    Lines = src_label_file.readlines()\n",
    "    bboxes = []\n",
    "    for line in Lines:\n",
    "        line = line.split(',')\n",
    "        line = [int(x) for x in line[:8]]\n",
    "        bboxes.append(line)\n",
    "    bb_c = bb_coordinates_xx_xy(bboxes, 'xy')\n",
    "    for bbx in bb_c:\n",
    "        (class_id, NorX, NorY, NorW, NorH) = pascal_voc_to_yolo(bbx[\"x_min\"], bbx[\"y_min\"], bbx[\"x_max\"], bbx[\"y_max\"], image_w, image_h)\n",
    "        file.write(str(class_id))\n",
    "        file.write(' ')\n",
    "        file.write(str(NorX))\n",
    "        file.write(' ')\n",
    "        file.write(str(NorY))\n",
    "        file.write(' ')\n",
    "        file.write(str(NorW))\n",
    "        file.write(' ')\n",
    "        file.write(str(NorH))\n",
    "        file.write('\\n')\n",
    "    file.close()\n",
    "print(\"Annotations converted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbe7e47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch_1",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
